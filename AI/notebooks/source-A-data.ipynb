{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "566be9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working from: D:\\SOMNiA\\AI\n",
      "RAW_DIR: D:\\SOMNiA\\AI\\raw\n",
      "PROCESSED_DIR: D:\\SOMNiA\\AI\\processed\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Imports & robust paths (works whether notebook is at project root or in AI/notebooks)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Start from current working directory (where the notebook runs)\n",
    "BASE = Path.cwd()\n",
    "\n",
    "# If we don't see raw/ here, but we do see it one level up (typical AI/notebooks layout), go up\n",
    "if not (BASE / \"raw\").exists() and (BASE.parent / \"raw\").exists():\n",
    "    BASE = BASE.parent\n",
    "\n",
    "RAW_DIR = BASE / \"raw\"\n",
    "PROCESSED_DIR = BASE / \"processed\"\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Working from:\", BASE.resolve())\n",
    "print(\"RAW_DIR:\", RAW_DIR.resolve())\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR.resolve())\n",
    "\n",
    "EXPECTED_COLS = [\n",
    "    \"DATE\",\"DAY_OF_THE_WEEK\",\"MONTH\",\"SEASON\",\n",
    "    \"ACTIVITY_steps\",\"ACTIVITY_distance\",\"ACTIVITY_soft\",\"ACTIVITY_moderate\",\"ACTIVITY_intense\",\n",
    "    \"ACTIVITY_hr_average\",\"ACTIVITY_hr_min\",\"ACTIVITY_hr_max\",\n",
    "    \"SLEEP_totalsleeptime\",\"SLEEP_durationtosleep\",\n",
    "    \"SLEEP_hr_average\",\"SLEEP_hr_min\",\"SLEEP_hr_max\",\n",
    "    \"SLEEP_wakeupcount\",\"SLEEP_wakeupduration\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0209186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 CSVs in d:\\SOMNiA\\AI\\raw\n",
      " - 1487.csv\n",
      " - 2201.csv\n",
      " - 2210.csv\n",
      " - 3379.csv\n",
      " - 4891.csv\n",
      " - 5359.csv\n",
      " - 5544.csv\n",
      " - 6008.csv\n",
      " - 6777.csv\n",
      " - 7359.csv\n",
      " - 8176.csv\n",
      " - 9775.csv\n"
     ]
    }
   ],
   "source": [
    "# CELL 1.5: Verify we can see your CSVs\n",
    "csv_files = sorted(RAW_DIR.glob(\"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} CSVs in {RAW_DIR}\")\n",
    "for p in csv_files[:12]:\n",
    "    print(\" -\", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27edb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Generic helpers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if c == \"DATE\":\n",
    "            continue\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def drop_low_quality_rows(df: pd.DataFrame, threshold: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"Drop rows where more than 'threshold' fraction of non-DATE columns are NaN.\"\"\"\n",
    "    non_date = df.drop(columns=[\"DATE\"], errors=\"ignore\")\n",
    "    frac_nan = non_date.isna().mean(axis=1)\n",
    "    return df.loc[frac_nan <= threshold].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d88259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Plausibility rules for activity\n",
    "\n",
    "def enforce_activity_rules(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [\"ACTIVITY_steps\",\"ACTIVITY_distance\",\"ACTIVITY_soft\",\"ACTIVITY_moderate\",\"ACTIVITY_intense\"]:\n",
    "        if c in df:\n",
    "            df.loc[df[c] < 0, c] = np.nan\n",
    "\n",
    "    for c in [\"ACTIVITY_hr_min\",\"ACTIVITY_hr_average\",\"ACTIVITY_hr_max\"]:\n",
    "        if c in df:\n",
    "            df.loc[(df[c] < 30) | (df[c] > 220), c] = np.nan\n",
    "\n",
    "    trio = {\"ACTIVITY_hr_min\",\"ACTIVITY_hr_average\",\"ACTIVITY_hr_max\"}\n",
    "    if trio.issubset(df.columns):\n",
    "        mask_bad = (\n",
    "            (df[\"ACTIVITY_hr_min\"] > df[\"ACTIVITY_hr_average\"]) |\n",
    "            (df[\"ACTIVITY_hr_average\"] > df[\"ACTIVITY_hr_max\"]) |\n",
    "            (df[\"ACTIVITY_hr_min\"] > df[\"ACTIVITY_hr_max\"])\n",
    "        )\n",
    "        df.loc[mask_bad, list(trio)] = np.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e72be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Plausibility rules for sleep\n",
    "\n",
    "def enforce_sleep_rules(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in [\"SLEEP_totalsleeptime\",\"SLEEP_durationtosleep\",\"SLEEP_wakeupduration\"]:\n",
    "        if c in df:\n",
    "            df.loc[(df[c] < 0) | (df[c] > 16*3600), c] = np.nan\n",
    "\n",
    "    for c in [\"SLEEP_hr_min\",\"SLEEP_hr_average\",\"SLEEP_hr_max\"]:\n",
    "        if c in df:\n",
    "            df.loc[(df[c] < 30) | (df[c] > 220), c] = np.nan\n",
    "\n",
    "    trio = {\"SLEEP_hr_min\",\"SLEEP_hr_average\",\"SLEEP_hr_max\"}\n",
    "    if trio.issubset(df.columns):\n",
    "        mask_bad = (\n",
    "            (df[\"SLEEP_hr_min\"] > df[\"SLEEP_hr_average\"]) |\n",
    "            (df[\"SLEEP_hr_average\"] > df[\"SLEEP_hr_max\"]) |\n",
    "            (df[\"SLEEP_hr_min\"] > df[\"SLEEP_hr_max\"])\n",
    "        )\n",
    "        df.loc[mask_bad, list(trio)] = np.nan\n",
    "\n",
    "    if \"SLEEP_wakeupcount\" in df:\n",
    "        df.loc[df[\"SLEEP_wakeupcount\"] < 0, \"SLEEP_wakeupcount\"] = np.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59492c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Imputation (rolling median ‚Üí person median ‚Üí global median) and winsorization\n",
    "\n",
    "def rolling_then_median_impute(df: pd.DataFrame, window: int = 7) -> pd.DataFrame:\n",
    "    cols = [c for c in df.columns if c not in [\"DATE\",\"person_id\"]]\n",
    "    global_medians = df[cols].median(numeric_only=True)\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        roll = s.rolling(window=window, min_periods=1).median()\n",
    "        s = s.fillna(roll)\n",
    "        s = s.fillna(s.median())\n",
    "        if s.isna().any():\n",
    "            s = s.fillna(global_medians.get(c, np.nan))\n",
    "        df[c] = s\n",
    "    return df\n",
    "\n",
    "def winsorize_per_person(df: pd.DataFrame, caps=(0.01, 0.99)) -> pd.DataFrame:\n",
    "    cols = [c for c in df.columns if c not in [\"DATE\",\"person_id\"]]\n",
    "    for c in cols:\n",
    "        q_low, q_hi = df[c].quantile(caps[0]), df[c].quantile(caps[1])\n",
    "        if pd.isna(q_low) or pd.isna(q_hi):\n",
    "            continue\n",
    "        df[c] = df[c].clip(q_low, q_hi)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f2258f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Clean one person's file robustly\n",
    "\n",
    "def clean_single_file(path: Path) -> tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Returns (cleaned_df, rows_dropped).\"\"\"\n",
    "    person_id = path.stem\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")  # handle BOMs\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    if \"DATE\" not in df.columns:\n",
    "        raise ValueError(f\"{path.name}: missing DATE column\")\n",
    "\n",
    "    keep = [\"DATE\"] + [c for c in EXPECTED_COLS if c != \"DATE\" and c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n",
    "    df = df.dropna(subset=[\"DATE\"]).sort_values(\"DATE\").drop_duplicates(subset=[\"DATE\"], keep=\"last\")\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"{path.name}: no valid DATE rows after parsing.\")\n",
    "\n",
    "    df = coerce_numeric(df)\n",
    "    df = enforce_activity_rules(df)\n",
    "    df = enforce_sleep_rules(df)\n",
    "\n",
    "    before = len(df)\n",
    "    df = drop_low_quality_rows(df, threshold=0.4)\n",
    "    dropped = before - len(df)\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"{path.name}: all rows dropped by quality threshold.\")\n",
    "\n",
    "    df.insert(1, \"person_id\", person_id)\n",
    "    df = rolling_then_median_impute(df, window=7)\n",
    "    df = winsorize_per_person(df, caps=(0.01, 0.99))\n",
    "\n",
    "    for col in EXPECTED_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    ordered = [\"DATE\",\"person_id\"] + [c for c in EXPECTED_COLS if c != \"DATE\"]\n",
    "    df = df[ordered]\n",
    "    return df, dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78582ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 raw files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/12 ‚úÖ 1487.csv ‚Üí 1487_clean.csv (dropped 0)\n",
      "  2/12 ‚úÖ 2201.csv ‚Üí 2201_clean.csv (dropped 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n",
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/12 ‚úÖ 2210.csv ‚Üí 2210_clean.csv (dropped 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/12 ‚úÖ 3379.csv ‚Üí 3379_clean.csv (dropped 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/12 ‚úÖ 4891.csv ‚Üí 4891_clean.csv (dropped 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/12 ‚úÖ 5359.csv ‚Üí 5359_clean.csv (dropped 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7/12 ‚úÖ 5544.csv ‚Üí 5544_clean.csv (dropped 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/12 ‚úÖ 6008.csv ‚Üí 6008_clean.csv (dropped 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9/12 ‚úÖ 6777.csv ‚Üí 6777_clean.csv (dropped 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/12 ‚úÖ 7359.csv ‚Üí 7359_clean.csv (dropped 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n",
      "C:\\Users\\ryanj\\AppData\\Local\\Temp\\ipykernel_18916\\2812088600.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11/12 ‚úÖ 8176.csv ‚Üí 8176_clean.csv (dropped 6)\n",
      " 12/12 ‚úÖ 9775.csv ‚Üí 9775_clean.csv (dropped 0)\n",
      "\n",
      "üßæ Saved QC summary: d:\\SOMNiA\\AI\\processed\\clean_qc_summary.csv\n",
      "üì¶ Saved merged dataset: d:\\SOMNiA\\AI\\processed\\merged_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>person_id</th>\n",
       "      <th>DAY_OF_THE_WEEK</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>ACTIVITY_steps</th>\n",
       "      <th>ACTIVITY_distance</th>\n",
       "      <th>ACTIVITY_soft</th>\n",
       "      <th>ACTIVITY_moderate</th>\n",
       "      <th>ACTIVITY_intense</th>\n",
       "      <th>ACTIVITY_hr_average</th>\n",
       "      <th>ACTIVITY_hr_min</th>\n",
       "      <th>ACTIVITY_hr_max</th>\n",
       "      <th>SLEEP_totalsleeptime</th>\n",
       "      <th>SLEEP_durationtosleep</th>\n",
       "      <th>SLEEP_hr_average</th>\n",
       "      <th>SLEEP_hr_min</th>\n",
       "      <th>SLEEP_hr_max</th>\n",
       "      <th>SLEEP_wakeupcount</th>\n",
       "      <th>SLEEP_wakeupduration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>1487</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2584.0</td>\n",
       "      <td>1841.0</td>\n",
       "      <td>16801.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>24960.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>1487</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4594.0</td>\n",
       "      <td>3201.0</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>24540.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-08</td>\n",
       "      <td>1487</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9602.0</td>\n",
       "      <td>7011.0</td>\n",
       "      <td>18242.0</td>\n",
       "      <td>4439.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>25680.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>1487</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14121.0</td>\n",
       "      <td>11017.0</td>\n",
       "      <td>18962.0</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>24180.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-10</td>\n",
       "      <td>1487</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7022.0</td>\n",
       "      <td>4835.0</td>\n",
       "      <td>20759.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>27120.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE person_id  DAY_OF_THE_WEEK  MONTH  SEASON  ACTIVITY_steps  \\\n",
       "0 2021-07-06      1487                1      6       0          2584.0   \n",
       "1 2021-07-07      1487                2      6       0          4594.0   \n",
       "2 2021-07-08      1487                3      6       0          9602.0   \n",
       "3 2021-07-09      1487                4      6       0         14121.0   \n",
       "4 2021-07-10      1487                5      6       0          7022.0   \n",
       "\n",
       "   ACTIVITY_distance  ACTIVITY_soft  ACTIVITY_moderate  ACTIVITY_intense  \\\n",
       "0             1841.0        16801.0               60.0               0.0   \n",
       "1             3201.0        19500.0              600.0               0.0   \n",
       "2             7011.0        18242.0             4439.0               0.0   \n",
       "3            11017.0        18962.0             4259.0            1500.0   \n",
       "4             4835.0        20759.0              661.0               0.0   \n",
       "\n",
       "   ACTIVITY_hr_average  ACTIVITY_hr_min  ACTIVITY_hr_max  \\\n",
       "0                 72.0             61.0            114.0   \n",
       "1                 72.0             61.0            111.0   \n",
       "2                 78.0             58.0            148.0   \n",
       "3                 76.0             60.0            110.0   \n",
       "4                 74.0             53.0            113.0   \n",
       "\n",
       "   SLEEP_totalsleeptime  SLEEP_durationtosleep  SLEEP_hr_average  \\\n",
       "0               24960.0                  120.0              64.0   \n",
       "1               24540.0                  120.0              63.0   \n",
       "2               25680.0                  120.0              60.0   \n",
       "3               24180.0                  120.0              60.0   \n",
       "4               27120.0                  480.0              63.0   \n",
       "\n",
       "   SLEEP_hr_min  SLEEP_hr_max  SLEEP_wakeupcount  SLEEP_wakeupduration  \n",
       "0          60.0          71.0                0.0                 180.0  \n",
       "1          59.0          70.0                0.0                 120.0  \n",
       "2          55.0          66.0                0.0                 180.0  \n",
       "3          54.0          68.0                1.0                 420.0  \n",
       "4          59.0          71.0                0.0                 840.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 7: Batch over raw/*.csv; save outputs and a QC report (diagnostic + resilient)\n",
    "\n",
    "import traceback\n",
    "\n",
    "all_clean = []\n",
    "qc_rows = []\n",
    "\n",
    "csv_files = sorted(RAW_DIR.glob(\"*.csv\"))\n",
    "print(f\"Found {len(csv_files)} raw files\\n\")\n",
    "\n",
    "for i, csv_path in enumerate(csv_files, 1):\n",
    "    try:\n",
    "        cleaned, dropped = clean_single_file(csv_path)\n",
    "        all_clean.append(cleaned)\n",
    "\n",
    "        out_path = PROCESSED_DIR / f\"{csv_path.stem}_clean.csv\"\n",
    "        cleaned.to_csv(out_path, index=False)\n",
    "\n",
    "        qc_rows.append({\n",
    "            \"file\": csv_path.name,\n",
    "            \"person_id\": csv_path.stem,\n",
    "            \"status\": \"ok\",\n",
    "            \"rows_after_clean\": len(cleaned),\n",
    "            \"rows_dropped\": dropped,\n",
    "            \"na_after_clean_total\": int(cleaned.isna().sum().sum()),\n",
    "            \"note\": \"\"\n",
    "        })\n",
    "        print(f\"{i:>3}/{len(csv_files)} ‚úÖ {csv_path.name} ‚Üí {out_path.name} (dropped {dropped})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        tb_first = traceback.format_exc().strip().splitlines()[-1]\n",
    "        qc_rows.append({\n",
    "            \"file\": csv_path.name,\n",
    "            \"person_id\": csv_path.stem,\n",
    "            \"status\": \"error\",\n",
    "            \"rows_after_clean\": 0,\n",
    "            \"rows_dropped\": None,\n",
    "            \"na_after_clean_total\": None,\n",
    "            \"note\": f\"{type(e).__name__}: {e} | {tb_first}\"\n",
    "        })\n",
    "        print(f\"{i:>3}/{len(csv_files)} ‚ùå {csv_path.name} failed: {e}\")\n",
    "\n",
    "qc = pd.DataFrame(qc_rows)\n",
    "qc_path = PROCESSED_DIR / \"clean_qc_summary.csv\"\n",
    "qc.to_csv(qc_path, index=False)\n",
    "print(f\"\\nüßæ Saved QC summary: {qc_path}\")\n",
    "\n",
    "if len(all_clean) > 0:\n",
    "    merged = pd.concat(all_clean, ignore_index=True).sort_values([\"person_id\",\"DATE\"])\n",
    "    merged_path = PROCESSED_DIR / \"merged_clean.csv\"\n",
    "    merged.to_csv(merged_path, index=False)\n",
    "    print(f\"üì¶ Saved merged dataset: {merged_path}\")\n",
    "    display(merged.head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No files were cleaned successfully. Open the QC summary to inspect errors:\")\n",
    "    display(qc.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac5c33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>person_id</th>\n",
       "      <th>status</th>\n",
       "      <th>rows_after_clean</th>\n",
       "      <th>rows_dropped</th>\n",
       "      <th>na_after_clean_total</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1487.csv</td>\n",
       "      <td>1487</td>\n",
       "      <td>ok</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2201.csv</td>\n",
       "      <td>2201</td>\n",
       "      <td>ok</td>\n",
       "      <td>282</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2210.csv</td>\n",
       "      <td>2210</td>\n",
       "      <td>ok</td>\n",
       "      <td>280</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3379.csv</td>\n",
       "      <td>3379</td>\n",
       "      <td>ok</td>\n",
       "      <td>286</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4891.csv</td>\n",
       "      <td>4891</td>\n",
       "      <td>ok</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5359.csv</td>\n",
       "      <td>5359</td>\n",
       "      <td>ok</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5544.csv</td>\n",
       "      <td>5544</td>\n",
       "      <td>ok</td>\n",
       "      <td>283</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6008.csv</td>\n",
       "      <td>6008</td>\n",
       "      <td>ok</td>\n",
       "      <td>288</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6777.csv</td>\n",
       "      <td>6777</td>\n",
       "      <td>ok</td>\n",
       "      <td>285</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7359.csv</td>\n",
       "      <td>7359</td>\n",
       "      <td>ok</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8176.csv</td>\n",
       "      <td>8176</td>\n",
       "      <td>ok</td>\n",
       "      <td>284</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9775.csv</td>\n",
       "      <td>9775</td>\n",
       "      <td>ok</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file  person_id status  rows_after_clean  rows_dropped  \\\n",
       "0   1487.csv       1487     ok               290             0   \n",
       "1   2201.csv       2201     ok               282             8   \n",
       "2   2210.csv       2210     ok               280            10   \n",
       "3   3379.csv       3379     ok               286             4   \n",
       "4   4891.csv       4891     ok               290             0   \n",
       "5   5359.csv       5359     ok               289             1   \n",
       "6   5544.csv       5544     ok               283             7   \n",
       "7   6008.csv       6008     ok               288             2   \n",
       "8   6777.csv       6777     ok               285             5   \n",
       "9   7359.csv       7359     ok               290             0   \n",
       "10  8176.csv       8176     ok               284             6   \n",
       "11  9775.csv       9775     ok               290             0   \n",
       "\n",
       "    na_after_clean_total  note  \n",
       "0                      0   NaN  \n",
       "1                      0   NaN  \n",
       "2                      0   NaN  \n",
       "3                      0   NaN  \n",
       "4                      0   NaN  \n",
       "5                      0   NaN  \n",
       "6                      0   NaN  \n",
       "7                      0   NaN  \n",
       "8                      0   NaN  \n",
       "9                      0   NaN  \n",
       "10                     0   NaN  \n",
       "11                     0   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'activity_hr_bad_rows': 0, 'sleep_hr_bad_rows': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 8: Sanity checks (load merged if needed)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if \"merged\" not in globals():\n",
    "    merged_path = Path(\"processed/merged_clean.csv\")\n",
    "    if not merged_path.exists():\n",
    "        raise FileNotFoundError(\"Run Cell 7 first to produce processed/merged_clean.csv\")\n",
    "    merged = pd.read_csv(merged_path, parse_dates=[\"DATE\"])\n",
    "\n",
    "qc_path = Path(\"../processed/clean_qc_summary.csv\")\n",
    "if qc_path.exists():\n",
    "    qc = pd.read_csv(qc_path)\n",
    "    display(qc.head(15))\n",
    "else:\n",
    "    print(\"QC summary not found.\")\n",
    "\n",
    "# 1) Non-negative checks\n",
    "nonneg_cols = [\n",
    "    \"ACTIVITY_steps\",\"ACTIVITY_distance\",\"ACTIVITY_soft\",\"ACTIVITY_moderate\",\"ACTIVITY_intense\",\n",
    "    \"SLEEP_totalsleeptime\",\"SLEEP_durationtosleep\",\"SLEEP_wakeupcount\",\"SLEEP_wakeupduration\",\n",
    "]\n",
    "bad = {c: int((merged[c] < 0).sum()) for c in nonneg_cols if c in merged.columns}\n",
    "bad\n",
    "\n",
    "# 2) HR ordering checks\n",
    "def count_bad_trio(df, prefix):\n",
    "    trio = [f\"{prefix}_hr_min\", f\"{prefix}_hr_average\", f\"{prefix}_hr_max\"]\n",
    "    if not all(c in df.columns for c in trio):\n",
    "        return None\n",
    "    return int(((df[trio[0]] > df[trio[1]]) |\n",
    "                (df[trio[1]] > df[trio[2]]) |\n",
    "                (df[trio[0]] > df[trio[2]])).sum())\n",
    "\n",
    "{\n",
    "    \"activity_hr_bad_rows\": count_bad_trio(merged, \"ACTIVITY\"),\n",
    "    \"sleep_hr_bad_rows\": count_bad_trio(merged, \"SLEEP\")\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
